install.packages("ggplot2")
install.packages("data.table")
install.packages("dplyr")
install.packages("caret")
install.packages("lattice")
library("lattice", lib.loc="~/R/win-library/3.3")
library("ggplot2", lib.loc="~/R/win-library/3.3")
detach("package:ggplot2", unload=TRUE)
detach("package:lattice", unload=TRUE)
install.packages("stringr")
install.packages("data.table")
install.packages("zoo")
install.packages("data.table")
remove(list = ls())
# Poker and roulette winnings from Monday to Friday:
poker_vector <- c(140, -50, 20, -120, 240)
roulette_vector <- c(-24, -50, 100, -350, 10)
days_vector <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
names(poker_vector) <- days_vector
names(roulette_vector) <- days_vector
# Which days did you make money on roulette?
selection_vector <- roulette_vector > 0
roulette_winning_days <- roulette_vector[selection_vector]
total_win_roulette <- sum(roulette_winning_days)
# Which days did you make money on poker?
selection_vector <- poker_vector > 0
poker_winning_days <- poker_vector[selection_vector]
total_win_poker <- sum(poker_winning_days)
#Print results
roulette_winning_days
total_win_roulette
poker_winning_days
total_win_poker
getwd()
rm(list = ls())
install.packages("installr")
library(installr)
updateR()
rm(list = ls())
rm(list = ls())
rm(list = ls())
version
version()
rversion)
rversion()
highlyCorrelated <- caret::findCorrelation(correlationMatrix, cutoff = 0.3)
library(caret)
library(ggplot2)
library(dummies)
#Importing dataset
df <- read.csv("HR_data.csv", header = TRUE)
#Renaming columns and cleaning the dataset
colnames(df)[colnames(df) == "sales"] <- "department"
#Creating factors, sampling and creating dummy variables
df$department <- as.factor(df$department)
df$salary <- as.factor(df$salary)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
dum.colnames <- c("department", "salary")
df.train <- dummy.data.frame(data = df.train, names = dum.colnames, drop = TRUE, sep = "_")
df.test <- dummy.data.frame(data = df.test, names = dum.colnames, drop = TRUE, sep = "_")
#Feature selection
correlationMatrix <- cor(df.train)
highlyCorrelated <- caret::findCorrelation(correlationMatrix, cutoff = 0.3, verbose = TRUE)
rm(list = ls())
library(caret)
library(ggplot2)
library(dummies)
library(pROC)
library(randomForest)
#Importing dataset
df <- read.csv("HR_data.csv", header = TRUE)
#Renaming columns and cleaning the dataset
colnames(df)[colnames(df) == "sales"] <- "department"
#Creating factors and partitions(based on department)
df$department <- as.factor(df$department)
df$salary <- as.factor(df$salary)
df$left <- as.factor(df$left)
set.seed(123457)
setwd("~/GitHub/Employee-attrition-prediction")
library(caret)
library(ggplot2)
library(dummies)
library(pROC)
library(randomForest)
library(Hmisc)
library(data.table)
#Importing dataset
df <- read.csv("HR_data.csv", header = TRUE)
#Renaming columns and cleaning the dataset
colnames(df)[colnames(df) == "sales"] <- "department"
#Creating factors and partitions
df$department <- as.factor(df$department)
df$salary <- as.factor(df$salary)
df$left <- as.factor(df$left)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
#Exploring collinearity
dum.colnames <- c("department", "salary")
df.train <- dummy.data.frame(data = df.train, names = dum.colnames, sep = "_")
df.test <- dummy.data.frame(data = df.test, names = dum.colnames, sep = "_")
cor.matrix <- Hmisc::rcorr(as.matrix(df.train))
print(cor.matrix)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
control <- caret::trainControl(method = "repeatedcv", number = 10, repeats = 3)
model <- caret::train(left~., data = df.train, method = "glm", trControl = control)
importance <- caret::varImp(model)
plot(importance)
imp <- setDT(importance$importance, keep.rownames = TRUE)
colnames(imp) <- c("Feature", "Importance")
imp <- imp[order(imp$Importance, decreasing = TRUE),]
print(imp)
fit2 <- glm(left ~ satisfaction_level + time_spend_company + Work_accident + salary + number_project
+ average_montly_hours, data = df.train, family = "binomial")
df.train$predicted_prob <- predict(fit2, df.train, type = "response")
df.test$predicted_prob <- predict(fit2, df.test, type = "response")
#Plot cutoff vs accuracy
accPlot.dataFrame <- data.frame(CUTOFF = seq(0, 1, length = 10000), ACCURACY = numeric(10000))
#Plot for training data set
for (index in 1:10000) {
pred <- ifelse((df.train$predicted_prob > accPlot.dataFrame[index, 1]), 1, 0)
true.positives <- sum(pred == 1 & df.train$left == 1)
true.negatives <- sum(pred == 0 & df.train$left == 0)
accPlot.dataFrame$ACCURACY[index] <- ((true.positives + true.negatives) / length(df.train$left)) * 100
}
ggplot2::ggplot(data = accPlot.dataFrame, mapping = aes(x = CUTOFF, y = ACCURACY, col)) + geom_line(size = 1)
idealCutoff <- accPlot.dataFrame$CUTOFF[which.max(accPlot.dataFrame$ACCURACY)]
acc.max <- accPlot.dataFrame$ACCURACY[which.max(accPlot.dataFrame$ACCURACY)]
idealCutoff
df.train$predicted_outcome <- ifelse((df.train$predicted_prob > idealCutoff), 1, 0)
df.test$predicted_outcome <- ifelse((df.test$predicted_prob > idealCutoff), 1, 0)
#Confusion matrices with optimal cutoff
conf.matrix.train <- caret::confusionMatrix(df.train$predicted_outcome, df.train$left)
conf.matrix.train
conf.matrix.test <- caret::confusionMatrix(df.test$predicted_outcome, df.test$left)
conf.matrix.test
#ROC for Logistic regerssion with selected predictors without dummy variables
roc.train <- pROC::roc(df.train$left, df.train$predicted_outcome)
pROC::plot.roc(roc.train)
roc.test1 <- pROC::roc(df.test$left, df.test$predicted_outcome)
pROC::plot.roc(roc.test1)
#With dummy variables
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
dum.colnames <- c("department", "salary")
df.train <- dummy.data.frame(data = df.train, names = dum.colnames, sep = "_")
df.test <- dummy.data.frame(data = df.test, names = dum.colnames, sep = "_")
control <- caret::trainControl(method = "repeatedcv", number = 10, repeats = 3)
model <- caret::train(left~., data = df.train, method = "glm", trControl = control)
importance <- caret::varImp(model)
plot(importance)
imp <- setDT(importance$importance, keep.rownames = TRUE)
colnames(imp) <- c("Feature", "Importance")
imp <- imp[order(imp$Importance, decreasing = TRUE),]
print(imp)
fit3 <- glm(left ~ satisfaction_level + time_spend_company + Work_accident + salary_low + number_project + salary_high
+ average_montly_hours, data = df.train, family = "binomial")
df.train$predicted_prob <- predict(fit3, df.train, type = "response")
df.test$predicted_prob <- predict(fit3, df.test, type = "response")
#Plot cutoff vs accuracy
#Plot for training data set
for (index in 1:10000) {
pred <- ifelse((df.train$predicted_prob > accPlot.dataFrame[index, 1]), 1, 0)
true.positives <- sum(pred == 1 & df.train$left == 1)
true.negatives <- sum(pred == 0 & df.train$left == 0)
accPlot.dataFrame$ACCURACY[index] <- ((true.positives + true.negatives) / length(df.train$left)) * 100
}
ggplot2::ggplot(data = accPlot.dataFrame, mapping = aes(x = CUTOFF, y = ACCURACY, col)) + geom_line(size = 1)
idealCutoff <- accPlot.dataFrame$CUTOFF[which.max(accPlot.dataFrame$ACCURACY)]
acc.max <- accPlot.dataFrame$ACCURACY[which.max(accPlot.dataFrame$ACCURACY)]
df.train$predicted_outcome <- ifelse((df.train$predicted_prob > idealCutoff), 1, 0)
df.test$predicted_outcome <- ifelse((df.test$predicted_prob > idealCutoff), 1, 0)
#Confusion matrices with optimal cutoff
conf.matrix.train <- caret::confusionMatrix(df.train$predicted_outcome, df.train$left)
conf.matrix.train
conf.matrix.test <- caret::confusionMatrix(df.test$predicted_outcome, df.test$left)
conf.matrix.test
#ROC for Logistic regerssion with selected predictors with dummy variables
roc.train <- pROC::roc(df.train$left, df.train$predicted_outcome)
pROC::plot.roc(roc.train)
roc.test1 <- pROC::roc(df.test$left, df.test$predicted_outcome)
pROC::plot.roc(roc.test1)
rm(list = ls())
library(caret)
library(ggplot2)
library(dummies)
library(pROC)
library(randomForest)
#Importing dataset
df <- read.csv("HR_data.csv", header = TRUE)
#Renaming columns and cleaning the dataset
colnames(df)[colnames(df) == "sales"] <- "department"
#Creating factors and partitions
df$department <- as.factor(df$department)
df$salary <- as.factor(df$salary)
df$left <- as.factor(df$left)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
#Random Forests
model.rf <- randomForest::randomForest(left ~ ., data = df.train, verbose = TRUE)
df.train$predicted_outcome <- predict(model.rf, newdata = df.train)
df.test$predicted_outcome <- predict(model.rf, newdata = df.test)
#Confusion matrices
conf.matrix.train <- caret::confusionMatrix(df.train$predicted_outcome, df.train$left)
conf.matrix.train
conf.matrix.test <- caret::confusionMatrix(df.test$predicted_outcome, df.test$left)
conf.matrix.test
#Feature selection in Random Forest (Backward Selection)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
control <- caret::rfeControl(functions = rfFuncs, method = "cv", number = 10)
results <- caret::rfe(df.train[,-(which(colnames(df.train) == "left"))], df.train$left, sizes = c(1:9), rfeControl = control, verbose = TRUE)
print(results)
ggplot(data = results) + geom_line()
vars.imp <- randomForest::importance(results$fit)
print(vars.imp)
varImpPlot(results$fit, sort = TRUE, main = "Feature Importance")
imp <- setDT(vars.imp, keep.rownames = TRUE)
str(vars.imp)
imp <- setDT(data.frame(vars.imp), keep.rownames = TRUE)
colnames(imp) <- c("Feature", "Mean_Decrease_Gini")
imp <- imp[order(imp$Mean_Decrease_Gini, decreasing = TRUE),]
print(imp)
ggplot(data = imp, mapping = aes(x = Feature, y = Mean_Decrease_Gini)) + geom_line()
ggplot(data = imp, mapping = aes(x = Feature, y = Mean_Decrease_Gini)) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point() + geom_line()
varImpPlot(results$fit, sort = TRUE, main = "Feature Importance")
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point() + geom_histogram()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point() + geom_hline()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature)) + geom_point() + geom_bar()
imp <- setDT(data.frame(vars.imp), keep.rownames = TRUE)
colnames(imp) <- c("Feature", "Mean_Decrease_Gini")
imp <- imp[order(imp$Mean_Decrease_Gini, decreasing = TRUE),]
print(imp)
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature )) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = Feature )) + geom_hline(yintercept = imp$Feature)
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, -Mean_Decrease_Gini))) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, Mean_Decrease_Gini))) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, Mean_Decrease_Gini))) + geom_bar() + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, Mean_Decrease_Gini))) + geom_point()
rm(list = ls())
library(caret)
library(ggplot2)
library(dummies)
library(pROC)
library(randomForest)
library(data.table)
#Importing dataset
df <- read.csv("HR_data.csv", header = TRUE)
#Renaming columns and cleaning the dataset
colnames(df)[colnames(df) == "sales"] <- "department"
#Creating factors and partitions
df$department <- as.factor(df$department)
df$salary <- as.factor(df$salary)
df$left <- as.factor(df$left)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
#IMPROVING THE MODEL
#Random Forests
model.rf <- randomForest::randomForest(left ~ ., data = df.train, verbose = TRUE)
df.train$predicted_outcome <- predict(model.rf, newdata = df.train)
df.test$predicted_outcome <- predict(model.rf, newdata = df.test)
#Confusion matrices
conf.matrix.train <- caret::confusionMatrix(df.train$predicted_outcome, df.train$left)
conf.matrix.train
conf.matrix.test <- caret::confusionMatrix(df.test$predicted_outcome, df.test$left)
conf.matrix.test
#Feature selection in Random Forest (Backward Selection)
set.seed(123457)
train_index <- caret::createDataPartition(df$department, p = 0.7, list = FALSE)
df.train <- df[train_index,]
df.test <- df[-train_index,]
control <- caret::rfeControl(functions = rfFuncs, method = "cv", number = 10)
results <- caret::rfe(df.train[,-(which(colnames(df.train) == "left"))], df.train$left, sizes = c(1:9), rfeControl = control, verbose = TRUE)
print(results)
ggplot(data = results) + geom_line()
vars.imp <- randomForest::importance(results$fit)
print(vars.imp)
imp <- setDT(data.frame(vars.imp), keep.rownames = TRUE)
colnames(imp) <- c("Feature", "Mean_Decrease_Gini")
imp <- imp[order(imp$Mean_Decrease_Gini, decreasing = TRUE),]
print(imp)
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, Mean_Decrease_Gini))) + geom_point()
ggplot(data = imp, mapping = aes(x = Mean_Decrease_Gini, y = reorder(Feature, Mean_Decrease_Gini))) + geom_point()
model.rf <- randomForest::randomForest(left ~ satisfaction_level + number_project + average_montly_hours + time_spend_company
+ last_evaluation + department, data = df.train, verbose = TRUE)
df.train$predicted_outcome <- predict(model.rf, newdata = df.train)
df.test$predicted_outcome <- predict(model.rf, newdata = df.test)
#Confusion matrices
conf.matrix.train <- caret::confusionMatrix(df.train$predicted_outcome, df.train$left)
conf.matrix.train
conf.matrix.test <- caret::confusionMatrix(df.test$predicted_outcome, df.test$left)
conf.matrix.test
rm(list = ls())
